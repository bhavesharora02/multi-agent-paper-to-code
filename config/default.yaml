# Default configuration for ML/DL Paper to Code system

# Pipeline Configuration
use_multi_agent_pipeline: true  # Set to true to use complete multi-agent pipeline
use_verification: true  # Set to true to enable verification agent (now improved with safety checks)
use_debugging: true  # Set to true to enable debugging agent

# PDF Parser Configuration
pdf_parser:
  method: "pdfplumber"  # Options: "pdfplumber", "pypdf2"
  max_pages: 100
  extract_images: false
  extract_tables: false
  
# Algorithm Extractor Configuration
extractor:
  # LLM Integration (set use_llm: true to enable)
  use_llm: true  # Set to true to use LLM-based extraction
  llm_provider: "groq"  # "openai", "anthropic", "openrouter", or "groq"
  model: "llama-3.3-70b-versatile"  # Groq model - fast and free tier available!
  fallback_to_rules: true  # Fall back to rule-based if LLM fails
  max_text_length: 8000  # Max characters to send to LLM
  
  # Rule-based extraction settings
  confidence_threshold: 0.3
  max_algorithms: 10
  include_pseudocode: true
  include_parameters: true
  include_complexity: true
  
  # Algorithm detection patterns
  patterns:
    neural_networks:
      - "neural network"
      - "deep learning"
      - "CNN"
      - "RNN"
      - "LSTM"
      - "GRU"
      - "Transformer"
    
    supervised_learning:
      - "linear regression"
      - "logistic regression"
      - "decision tree"
      - "random forest"
      - "SVM"
      - "naive bayes"
      - "k-nearest neighbors"
    
    unsupervised_learning:
      - "k-means"
      - "clustering"
      - "PCA"
      - "t-SNE"
      - "UMAP"
    
    optimization:
      - "gradient descent"
      - "Adam"
      - "SGD"
      - "RMSprop"

# Code Generator Configuration
generator:
  # LLM Integration (set use_llm: true to enable)
  use_llm: true  # Set to true to use LLM-based code generation
  llm_provider: "groq"  # "openai", "anthropic", "openrouter", or "groq"
  model: "llama-3.3-70b-versatile"  # Groq model - fast and free tier available!
  use_fallback: true  # Fall back to template-based if LLM fails
  
  # Template-based generation settings
  default_framework: "pytorch"
  include_docstrings: true
  include_tests: false
  include_examples: true
  
  # Framework-specific settings
  frameworks:
    pytorch:
      default_hidden_size: 128
      default_learning_rate: 0.001
      default_epochs: 100
      include_visualization: true
    
    tensorflow:
      default_hidden_units: 128
      default_learning_rate: 0.001
      default_epochs: 100
      include_visualization: true
    
    sklearn:
      default_test_size: 0.2
      include_preprocessing: true
      include_evaluation: true

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file: null  # Set to a file path to enable file logging

# Output Configuration
output:
  format_code: true
  max_line_length: 88
  include_imports: true
  include_main_function: true
